{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7650a7c8-bdb0-4649-bf5d-7ca5e2d22d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypergraph Constructed at Time=1 | Hyperedges: 22 | Layers: 3\n",
      "Generation 1/10 | Best Qh: -0.8000, Best Jc: 0.5000\n",
      "Generation 2/10 | Best Qh: -0.8000, Best Jc: 0.5000\n",
      "Generation 3/10 | Best Qh: -0.8000, Best Jc: 0.5000\n",
      "Generation 4/10 | Best Qh: -0.8000, Best Jc: 0.5000\n",
      "Generation 5/10 | Best Qh: -0.8000, Best Jc: 0.5000\n",
      "Generation 6/10 | Best Qh: -0.8000, Best Jc: 0.5000\n",
      "Generation 7/10 | Best Qh: -0.8000, Best Jc: 0.5000\n",
      "Generation 8/10 | Best Qh: -0.8000, Best Jc: 0.5000\n",
      "Generation 9/10 | Best Qh: -0.8000, Best Jc: 0.5000\n",
      "Generation 10/10 | Best Qh: -0.8000, Best Jc: 0.5000\n",
      "\n",
      "Optimization Complete! Optimal Individual Objectives:\n",
      "Hypergraph Modularity (Qh): -0.8000\n",
      "Inter-layer Consistency (Jc): 0.5000\n",
      "Dynamic Stability (Sd): 0.1423\n",
      "Resource Efficiency (Re): 9.6071\n",
      "Optimal Community Structure: {0: ['he_initial_0', 'he_initial_3', 'he_initial_4', 'he_initial_5', 'he_initial_17'], 1: ['he_initial_14', 'he_initial_18', 'he_initial_19', 'he_new_1_0'], 2: ['he_initial_2', 'he_initial_12', 'he_initial_15', 'he_new_1_1'], 3: ['he_initial_1', 'he_initial_7', 'he_initial_9'], 4: ['he_initial_6', 'he_initial_8', 'he_initial_10', 'he_initial_11', 'he_initial_13', 'he_initial_16']}\n",
      "\n",
      "Optimal Individual Genes:\n",
      "Structure Genes: [1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0]\n",
      "Community Genes: [0 3 2 0 0 0 4 3 4 3 4 4 2 4 1 2 4 0 1 1 1 2 4 4 0 0 1 0 2 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "import random\n",
    "\n",
    "\n",
    "# -------------------------- Algorithm 1: Dynamic Multi-Layer Hypergraph Construction -------------------------\n",
    "class DynamicHypergraph:\n",
    "    def __init__(self, nodes, layers, initial_hyperedges, time_windows):\n",
    "        self.nodes = nodes  # Set of entities {v1, ..., vn}\n",
    "        self.layers = layers  # Set of functional layers {l1, ..., lm}\n",
    "        self.current_hyperedges = initial_hyperedges  # Initial hyperedge set\n",
    "        self.time_windows = time_windows  # Time window sequence\n",
    "        # Intra-layer adjacency matrices (initialized by layer)\n",
    "        self.adj_matrices = {layer: np.zeros((len(nodes), len(nodes))) for layer in layers}\n",
    "        # Coupling tensor T (layer × layer × node)\n",
    "        self.coupling_tensor = np.zeros((len(layers), len(layers), len(nodes)))\n",
    "        # Index mappings (for matrix operations)\n",
    "        self.layer_to_idx = {layer: i for i, layer in enumerate(layers)}\n",
    "        self.node_to_idx = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "    def update_hyperedge_weights(self, current_time, lambda_short=0.4, lambda_long=0.15, prune_threshold=0.1):\n",
    "        \"\"\"Update hyperedge weights (exponential decay) and prune low-weight hyperedges\"\"\"\n",
    "        updated_hyperedges = {}\n",
    "        for he_id, (he_attrs, layer, create_time, weight) in self.current_hyperedges.items():\n",
    "            time_diff = current_time - create_time\n",
    "            # Select decay rate based on hyperedge type\n",
    "            if he_attrs.get('type') == 'transient':\n",
    "                new_weight = weight * np.exp(-lambda_short * time_diff)\n",
    "            else:\n",
    "                new_weight = weight * np.exp(-lambda_long * time_diff)\n",
    "            # Prune hyperedges with weight below threshold\n",
    "            if new_weight >= prune_threshold:\n",
    "                updated_hyperedges[he_id] = (he_attrs, layer, create_time, new_weight)\n",
    "        self.current_hyperedges = updated_hyperedges\n",
    "\n",
    "    def detect_new_events(self, current_time, event_rate=0.1):\n",
    "        \"\"\"Simulate new event detection (generate new hyperedges)\"\"\"\n",
    "        new_hyperedges = {}\n",
    "        new_event_count = max(1, int(len(self.current_hyperedges) * event_rate))\n",
    "        for i in range(new_event_count):\n",
    "            he_id = f\"he_new_{current_time}_{i}\"\n",
    "            layer = random.choice(list(self.layers))\n",
    "            selected_nodes = random.sample(self.nodes, k=random.randint(2, 5))\n",
    "            he_type = 'transient' if random.random() < 0.3 else 'strategic'\n",
    "            new_hyperedges[he_id] = (\n",
    "                {'nodes': selected_nodes, 'type': he_type}, \n",
    "                layer, \n",
    "                current_time, \n",
    "                1.0  # Initial weight\n",
    "            )\n",
    "        self.current_hyperedges.update(new_hyperedges)\n",
    "\n",
    "    def update_adjacency_matrices(self):\n",
    "        \"\"\"Update intra-layer adjacency matrices\"\"\"\n",
    "        for layer in self.layers:\n",
    "            adj_matrix = np.zeros((len(self.nodes), len(self.nodes)))\n",
    "            for (he_attrs, he_layer, create_time, weight) in self.current_hyperedges.values():\n",
    "                if he_layer != layer:\n",
    "                    continue\n",
    "                node_indices = [self.node_to_idx[node] for node in he_attrs['nodes']]\n",
    "                for i in node_indices:\n",
    "                    for j in node_indices:\n",
    "                        if i != j:\n",
    "                            adj_matrix[i, j] += weight\n",
    "            self.adj_matrices[layer] = adj_matrix\n",
    "\n",
    "    def update_coupling_tensor(self, community_structure, eta=0.1):\n",
    "        \"\"\"Update coupling tensor (inter-layer community correlation)\"\"\"\n",
    "        num_layers = len(self.layers)\n",
    "        for t_idx in range(num_layers):\n",
    "            for s_idx in range(num_layers):\n",
    "                if t_idx == s_idx:\n",
    "                    continue\n",
    "                layer_t = list(self.layers)[t_idx]\n",
    "                layer_s = list(self.layers)[s_idx]\n",
    "                comm_t = community_structure[layer_t]\n",
    "                comm_s = community_structure[layer_s]\n",
    "                intersect_nodes = set(comm_t) & set(comm_s)\n",
    "                for node in intersect_nodes:\n",
    "                    node_idx = self.node_to_idx[node]\n",
    "                    self.coupling_tensor[t_idx, s_idx, node_idx] += eta * len(intersect_nodes) / len(self.nodes)\n",
    "\n",
    "    def construct_hypergraph(self, current_time):\n",
    "        \"\"\"Complete hypergraph construction workflow\"\"\"\n",
    "        self.update_hyperedge_weights(current_time)\n",
    "        self.detect_new_events(current_time)\n",
    "        self.update_adjacency_matrices()\n",
    "        return self.current_hyperedges, self.adj_matrices, self.coupling_tensor\n",
    "\n",
    "\n",
    "# -------------------------- Algorithm 2: Multi-Objective Function Evaluation -------------------------\n",
    "def compute_hypergraph_modularity(hyperedges, community_structure, gamma=1.0, omega=1.0):\n",
    "    \"\"\"Compute hypergraph modularity (Qh)\"\"\"\n",
    "    total_weight = sum(weight for (he_attrs, layer, create_time, weight) in hyperedges.values())\n",
    "    if total_weight == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    modularity = 0.0\n",
    "    for (he_attrs, layer, create_time, weight) in hyperedges.values():\n",
    "        he_nodes = he_attrs['nodes']\n",
    "        comm_count = {comm_id: 0 for comm_id in community_structure.keys()}\n",
    "        for node in he_nodes:\n",
    "            for comm_id, comm_nodes in community_structure.items():\n",
    "                if node in comm_nodes:\n",
    "                    comm_count[comm_id] += 1\n",
    "                    break\n",
    "        dominant_comm = max(comm_count.keys(), key=lambda x: comm_count[x])\n",
    "        overlap_ratio = comm_count[dominant_comm] / len(he_nodes)\n",
    "        modularity += weight * (overlap_ratio - gamma * (len(he_nodes) / len(community_structure)) ** omega)\n",
    "    \n",
    "    return modularity / total_weight\n",
    "\n",
    "\n",
    "def compute_interlayer_consistency(community_structure):\n",
    "    \"\"\"Compute inter-layer consistency (Jc)\"\"\"\n",
    "    layers = list(community_structure.keys())\n",
    "    num_layers = len(layers)\n",
    "    if num_layers < 2:\n",
    "        return 1.0\n",
    "    \n",
    "    jaccard_sum = 0.0\n",
    "    for i in range(num_layers):\n",
    "        for j in range(i + 1, num_layers):\n",
    "            comm_i = community_structure[layers[i]]\n",
    "            comm_j = community_structure[layers[j]]\n",
    "            intersect = len(set(comm_i) & set(comm_j))\n",
    "            union = len(set(comm_i) | set(comm_j))\n",
    "            jaccard_sum += intersect / union if union > 0 else 0.0\n",
    "    avg_jaccard = jaccard_sum / (num_layers * (num_layers - 1) / 2)\n",
    "    \n",
    "    all_nodes = []\n",
    "    all_labels = []\n",
    "    for layer_idx, (layer, comm_nodes) in enumerate(community_structure.items()):\n",
    "        all_nodes.extend(comm_nodes)\n",
    "        all_labels.extend([layer_idx] * len(comm_nodes))\n",
    "    nmi = normalized_mutual_info_score(all_labels, all_labels)\n",
    "    \n",
    "    return (avg_jaccard + nmi) / 2\n",
    "\n",
    "\n",
    "def compute_dynamic_stability(prev_community, curr_community, eta=0.2):\n",
    "    \"\"\"Compute dynamic stability (Sd)\"\"\"\n",
    "    if not prev_community:\n",
    "        return 1.0\n",
    "    \n",
    "    prev_node_labels = {}\n",
    "    for comm_id, comm_nodes in prev_community.items():\n",
    "        for node in comm_nodes:\n",
    "            prev_node_labels[node] = comm_id\n",
    "    \n",
    "    curr_node_labels = {}\n",
    "    for comm_id, comm_nodes in curr_community.items():\n",
    "        for node in comm_nodes:\n",
    "            curr_node_labels[node] = comm_id\n",
    "    \n",
    "    common_nodes = set(prev_node_labels.keys()) & set(curr_node_labels.keys())\n",
    "    if not common_nodes:\n",
    "        return 0.0\n",
    "    \n",
    "    prev_labels = [prev_node_labels[node] for node in common_nodes]\n",
    "    curr_labels = [curr_node_labels[node] for node in common_nodes]\n",
    "    nmi = normalized_mutual_info_score(prev_labels, curr_labels)\n",
    "    \n",
    "    prev_comm_ids = set(prev_community.keys())\n",
    "    curr_comm_ids = set(curr_community.keys())\n",
    "    new_comm_count = len(curr_comm_ids - prev_comm_ids)\n",
    "    reorganization_penalty = eta * (new_comm_count / len(curr_comm_ids)) if len(curr_comm_ids) > 0 else 0.0\n",
    "    \n",
    "    return max(nmi - reorganization_penalty, 0.0)\n",
    "\n",
    "\n",
    "def compute_resource_efficiency(hyperedges, role_hierarchy):\n",
    "    \"\"\"Compute resource efficiency (Re)\"\"\"\n",
    "    efficiency = 0.0\n",
    "    alpha = 0.5\n",
    "    beta = 0.5\n",
    "    \n",
    "    for (he_attrs, layer, create_time, weight) in hyperedges.values():\n",
    "        he_nodes = he_attrs['nodes']\n",
    "        if len(he_nodes) < 2:\n",
    "            continue\n",
    "        \n",
    "        layer_span = 1\n",
    "        node_roles = [role_hierarchy.get(node, 1) for node in he_nodes]\n",
    "        role_diff = max(node_roles) - min(node_roles)\n",
    "        efficiency += weight * (alpha * layer_span + beta * role_diff)\n",
    "    \n",
    "    return efficiency\n",
    "\n",
    "\n",
    "def evaluate_multi_objectives(hypergraph, community_structure, prev_community, role_hierarchy):\n",
    "    \"\"\"Multi-objective evaluation\"\"\"\n",
    "    hyperedges, _, _ = hypergraph\n",
    "    qh = compute_hypergraph_modularity(hyperedges, community_structure)\n",
    "    jc = compute_interlayer_consistency(community_structure)\n",
    "    sd = compute_dynamic_stability(prev_community, community_structure)\n",
    "    re = compute_resource_efficiency(hyperedges, role_hierarchy)\n",
    "    return np.array([qh, jc, sd, re])\n",
    "\n",
    "\n",
    "# -------------------------- Algorithm 3: NSGA-III Community Detection -------------------------\n",
    "class NSGAIIICommunityDetector:\n",
    "    def __init__(self, hypergraph, pop_size=50, generations=10, crossover_prob=0.8, mutation_prob=0.2):\n",
    "        self.hypergraph = hypergraph\n",
    "        self.pop_size = pop_size\n",
    "        self.generations = generations\n",
    "        self.crossover_prob = crossover_prob\n",
    "        self.mutation_prob = mutation_prob\n",
    "        self.num_objectives = 4\n",
    "        # Generate reference points manually instead of using pymoo\n",
    "        self.reference_points = self.generate_reference_points(num_points=10)\n",
    "\n",
    "    def generate_reference_points(self, num_points=10):\n",
    "        \"\"\"Manually generate uniform reference points for 4 objectives\"\"\"\n",
    "        if self.num_objectives != 4:\n",
    "            raise ValueError(\"This implementation supports exactly 4 objectives\")\n",
    "            \n",
    "        points = []\n",
    "        # Create evenly distributed points in 4D space between [0,1]\n",
    "        for i in range(num_points + 1):\n",
    "            for j in range(num_points + 1 - i):\n",
    "                for k in range(num_points + 1 - i - j):\n",
    "                    l = num_points - i - j - k\n",
    "                    if l < 0:\n",
    "                        continue\n",
    "                    point = np.array([i, j, k, l]) / num_points\n",
    "                    points.append(point)\n",
    "                    if len(points) >= num_points:\n",
    "                        return np.array(points[:num_points])\n",
    "        return np.array(points)\n",
    "\n",
    "    def initialize_population(self, num_nodes, num_communities=5):\n",
    "        \"\"\"Initialize population\"\"\"\n",
    "        population = []\n",
    "        num_hyperedges = len(self.hypergraph[0])\n",
    "        \n",
    "        for _ in range(self.pop_size):\n",
    "            struct_genes = np.random.randint(0, 2, size=num_hyperedges)\n",
    "            comm_genes = np.random.randint(0, num_communities, size=num_nodes)\n",
    "            population.append((struct_genes, comm_genes))\n",
    "        \n",
    "        return population\n",
    "\n",
    "    def crossover_operation(self, parent1, parent2):\n",
    "        \"\"\"Crossover operation\"\"\"\n",
    "        struct1, comm1 = parent1\n",
    "        struct2, comm2 = parent2\n",
    "        \n",
    "        crossover_idx = np.random.randint(1, len(struct1))\n",
    "        child_struct = np.concatenate([struct1[:crossover_idx], struct2[crossover_idx:]])\n",
    "        \n",
    "        crossover_mask = np.random.randint(0, 2, size=len(comm1))\n",
    "        child_comm = np.where(crossover_mask == 1, comm1, comm2)\n",
    "        \n",
    "        return (child_struct, child_comm)\n",
    "\n",
    "    def mutation_operation(self, individual, num_communities=5):\n",
    "        \"\"\"Mutation operation\"\"\"\n",
    "        struct_genes, comm_genes = individual\n",
    "        \n",
    "        mutated_struct = np.where(\n",
    "            np.random.rand(len(struct_genes)) < self.mutation_prob,\n",
    "            1 - struct_genes,\n",
    "            struct_genes\n",
    "        )\n",
    "        \n",
    "        mutated_comm = comm_genes.copy()\n",
    "        if np.random.rand() < self.mutation_prob:\n",
    "            mutate_count = max(1, int(0.1 * len(comm_genes)))\n",
    "            mutate_nodes = np.random.choice(len(comm_genes), size=mutate_count, replace=False)\n",
    "            mutated_comm[mutate_nodes] = np.random.randint(0, num_communities, size=mutate_count)\n",
    "        \n",
    "        return (mutated_struct, mutated_comm)\n",
    "\n",
    "    def non_dominated_sorting(self, population, objectives):\n",
    "        \"\"\"Non-dominated sorting\"\"\"\n",
    "        num_individuals = len(population)\n",
    "        domination_count = [0] * num_individuals\n",
    "        dominated_by = [[] for _ in range(num_individuals)]\n",
    "        fronts = []\n",
    "        \n",
    "        for i in range(num_individuals):\n",
    "            for j in range(num_individuals):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                i_dominates_j = all(objectives[k][i] >= objectives[k][j] for k in range(self.num_objectives))\n",
    "                j_dominates_i = all(objectives[k][j] >= objectives[k][i] for k in range(self.num_objectives))\n",
    "                \n",
    "                if i_dominates_j and not j_dominates_i:\n",
    "                    dominated_by[i].append(j)\n",
    "                    domination_count[j] += 1\n",
    "        \n",
    "        current_front = [i for i in range(num_individuals) if domination_count[i] == 0]\n",
    "        \n",
    "        while current_front:\n",
    "            fronts.append(current_front)\n",
    "            next_front = []\n",
    "            for ind_idx in current_front:\n",
    "                for dominated_idx in dominated_by[ind_idx]:\n",
    "                    domination_count[dominated_idx] -= 1\n",
    "                    if domination_count[dominated_idx] == 0:\n",
    "                        next_front.append(dominated_idx)\n",
    "            current_front = next_front\n",
    "        \n",
    "        return fronts\n",
    "\n",
    "    def environmental_selection(self, population, objectives):\n",
    "        \"\"\"Environmental selection\"\"\"\n",
    "        fronts = self.non_dominated_sorting(population, objectives)\n",
    "        selected_indices = []\n",
    "        \n",
    "        for front in fronts:\n",
    "            if len(selected_indices) + len(front) <= self.pop_size:\n",
    "                selected_indices.extend(front)\n",
    "            else:\n",
    "                remaining_slots = self.pop_size - len(selected_indices)\n",
    "                front_objectives = np.array([\n",
    "                    [objectives[k][ind_idx] for k in range(self.num_objectives)] \n",
    "                    for ind_idx in front\n",
    "                ])\n",
    "                max_obj = np.max(front_objectives, axis=0, keepdims=True)\n",
    "                normalized_front = front_objectives / max_obj\n",
    "                normalized_ref = self.reference_points / max_obj\n",
    "                distances = np.sqrt(np.sum(\n",
    "                    (normalized_front[:, None, :] - normalized_ref[None, :, :]) ** 2, \n",
    "                    axis=2\n",
    "                ))\n",
    "                min_distances = np.min(distances, axis=1)\n",
    "                top_individuals = np.argsort(min_distances)[:remaining_slots]\n",
    "                selected_indices.extend([front[idx] for idx in top_individuals])\n",
    "                break\n",
    "        \n",
    "        return [population[idx] for idx in selected_indices]\n",
    "\n",
    "    def calculate_hypervolume(self, obj_values, ref_point):\n",
    "        \"\"\"Calculate hypervolume\"\"\"\n",
    "        return np.prod([obj - ref for obj, ref in zip(obj_values, ref_point)])\n",
    "\n",
    "    def optimize(self, num_nodes, num_communities=5):\n",
    "        \"\"\"Execute NSGA-III optimization\"\"\"\n",
    "        population = self.initialize_population(num_nodes, num_communities)\n",
    "        prev_community = {}\n",
    "        role_hierarchy = {node: random.randint(1, 4) for node in self.hypergraph[0].keys()}\n",
    "        \n",
    "        for gen in range(self.generations):\n",
    "            objectives = [[0.0 for _ in range(len(population))] for _ in range(self.num_objectives)]\n",
    "            for ind_idx, individual in enumerate(population):\n",
    "                struct_genes, comm_genes = individual\n",
    "                activated_hyperedges = {}\n",
    "                hyperedge_list = list(self.hypergraph[0].items())\n",
    "                for he_idx, (he_id, he_data) in enumerate(hyperedge_list):\n",
    "                    if struct_genes[he_idx] == 1:\n",
    "                        activated_hyperedges[he_id] = he_data\n",
    "                \n",
    "                community_struct = {c: [] for c in range(num_communities)}\n",
    "                node_list = list(self.hypergraph[0].keys())\n",
    "                for node_idx, comm_id in enumerate(comm_genes):\n",
    "                    if node_idx < len(node_list):\n",
    "                        community_struct[comm_id].append(node_list[node_idx])\n",
    "                community_struct = {c: nodes for c, nodes in community_struct.items() if len(nodes) > 0}\n",
    "                \n",
    "                if not community_struct:\n",
    "                    qh, jc, sd, re = 0.0, 0.0, 0.0, 1e6\n",
    "                else:\n",
    "                    qh = compute_hypergraph_modularity(activated_hyperedges, community_struct)\n",
    "                    jc = compute_interlayer_consistency(community_struct)\n",
    "                    sd = compute_dynamic_stability(prev_community, community_struct)\n",
    "                    re = compute_resource_efficiency(activated_hyperedges, role_hierarchy)\n",
    "                \n",
    "                objectives[0][ind_idx] = qh\n",
    "                objectives[1][ind_idx] = jc\n",
    "                objectives[2][ind_idx] = sd\n",
    "                objectives[3][ind_idx] = -re\n",
    "            \n",
    "            selected_parents = self.environmental_selection(population, objectives)\n",
    "            offspring = []\n",
    "            while len(offspring) < self.pop_size - len(selected_parents):\n",
    "                parent1, parent2 = random.sample(selected_parents, 2)\n",
    "                if random.random() < self.crossover_prob:\n",
    "                    child = self.crossover_operation(parent1, parent2)\n",
    "                else:\n",
    "                    child = parent1\n",
    "                child = self.mutation_operation(child, num_communities)\n",
    "                offspring.append(child)\n",
    "            \n",
    "            population = selected_parents + offspring\n",
    "            prev_community = community_struct.copy()\n",
    "            \n",
    "            print(f\"Generation {gen+1}/{self.generations} | Best Qh: {max(objectives[0]):.4f}, Best Jc: {max(objectives[1]):.4f}\")\n",
    "        \n",
    "        final_objectives = []\n",
    "        final_communities = []\n",
    "        for individual in population:\n",
    "            struct_genes, comm_genes = individual\n",
    "            community_struct = {c: [] for c in range(num_communities)}\n",
    "            node_list = list(self.hypergraph[0].keys())\n",
    "            for node_idx, comm_id in enumerate(comm_genes):\n",
    "                if node_idx < len(node_list):\n",
    "                    community_struct[comm_id].append(node_list[node_idx])\n",
    "            community_struct = {c: nodes for c, nodes in community_struct.items() if len(nodes) > 0}\n",
    "            final_communities.append(community_struct)\n",
    "            \n",
    "            qh = compute_hypergraph_modularity(self.hypergraph[0], community_struct)\n",
    "            jc = compute_interlayer_consistency(community_struct)\n",
    "            sd = compute_dynamic_stability(prev_community, community_struct)\n",
    "            re = compute_resource_efficiency(self.hypergraph[0], role_hierarchy)\n",
    "            final_objectives.append([qh, jc, sd, re])\n",
    "        \n",
    "        final_objs_np = np.array(final_objectives)\n",
    "        ref_point = np.min(final_objs_np, axis=0)\n",
    "        final_objs_for_hv = final_objs_np.copy()\n",
    "        final_objs_for_hv[:, 3] = ref_point[3] - final_objs_for_hv[:, 3]\n",
    "        hv_values = [self.calculate_hypervolume(obj, ref_point) for obj in final_objs_for_hv]\n",
    "        best_idx = np.argmax(hv_values)\n",
    "        best_individual = population[best_idx]\n",
    "        best_community = final_communities[best_idx]\n",
    "        \n",
    "        print(\"\\nOptimization Complete! Optimal Individual Objectives:\")\n",
    "        print(f\"Hypergraph Modularity (Qh): {final_objectives[best_idx][0]:.4f}\")\n",
    "        print(f\"Inter-layer Consistency (Jc): {final_objectives[best_idx][1]:.4f}\")\n",
    "        print(f\"Dynamic Stability (Sd): {final_objectives[best_idx][2]:.4f}\")\n",
    "        print(f\"Resource Efficiency (Re): {final_objectives[best_idx][3]:.4f}\")\n",
    "        print(f\"Optimal Community Structure: {best_community}\")\n",
    "        \n",
    "        return best_individual, best_community\n",
    "\n",
    "\n",
    "# -------------------------- Test Code (Main Function) -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    num_nodes = 30\n",
    "    nodes = [f\"node_{i}\" for i in range(num_nodes)]\n",
    "    layers = [\"R&D\", \"Marketing\", \"Management\"]\n",
    "    initial_hyperedges = {}\n",
    "    for i in range(20):\n",
    "        he_id = f\"he_initial_{i}\"\n",
    "        selected_nodes = random.sample(nodes, k=4)\n",
    "        layer = layers[i % 3]\n",
    "        initial_hyperedges[he_id] = (\n",
    "            {'nodes': selected_nodes, 'type': 'strategic'},\n",
    "            layer,\n",
    "            0,\n",
    "            1.0\n",
    "        )\n",
    "    time_windows = [0, 1, 2, 3]\n",
    "    \n",
    "    # Construct dynamic hypergraph\n",
    "    hypergraph_builder = DynamicHypergraph(nodes, layers, initial_hyperedges, time_windows)\n",
    "    hypergraph_data = hypergraph_builder.construct_hypergraph(current_time=1)\n",
    "    print(f\"Hypergraph Constructed at Time=1 | Hyperedges: {len(hypergraph_data[0])} | Layers: {len(hypergraph_data[1])}\")\n",
    "    \n",
    "    # Run NSGA-III community detection\n",
    "    nsga3_detector = NSGAIIICommunityDetector(\n",
    "        hypergraph=hypergraph_data,\n",
    "        pop_size=50,\n",
    "        generations=10,\n",
    "        crossover_prob=0.8,\n",
    "        mutation_prob=0.2\n",
    "    )\n",
    "    optimal_individual, optimal_community = nsga3_detector.optimize(num_nodes=num_nodes, num_communities=5)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nOptimal Individual Genes:\")\n",
    "    print(f\"Structure Genes: {optimal_individual[0]}\")\n",
    "    print(f\"Community Genes: {optimal_individual[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f15244-2466-41db-b82e-246da758ffad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
